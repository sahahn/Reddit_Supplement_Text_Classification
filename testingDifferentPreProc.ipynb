{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sage Hahn\n",
    "\n",
    "**Data Science Final Project**\n",
    "\n",
    "Testing different pre-processing techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now I have a few main sources of text, reddit posts and a pubically avaliable sample of tweets, clips from news articles and blog posts. These sources are in a few different formats, and contain text of varying length and usefulness. I will begin by only loading one dataset, lets say the news clips from coursera, and will use it to explore which pre-processing steps I would like to apply universally to each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "read_path = 'data/courseraNews.txt'\n",
    "\n",
    "news_posts = []\n",
    "\n",
    "with open(read_path, 'r') as f:\n",
    "    news_posts = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1010242"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright there are a little over a million samples of writing here. The first thing I'd like to try is using the sentence tokenizer via the NLTK library, in order to split the dataset up into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "for line in news_posts:\n",
    "    sent_text = nltk.sent_tokenize(line)\n",
    "    \n",
    "    for sent in sent_text:\n",
    "        sentences.append(sent.lower())  #Also change to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1950024"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears this set of news data was already broken down a little bit, as the size once turned into sentences only roughly doubled. Lets not try with the much rougher reddit data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "read_path = 'data/redditTextBlock1.pkl'\n",
    "\n",
    "#Read in the first post chunk\n",
    "posts = []\n",
    "\n",
    "with open(read_path, 'rb') as f:\n",
    "    posts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reddit_sentences = []\n",
    "\n",
    "for line in posts[:1000000]: #Only investigating w/ one million vs all 10 million\n",
    "    sent_text = nltk.sent_tokenize(line)\n",
    "    \n",
    "    for sent in sent_text:\n",
    "        reddit_sentences.append(sent.lower()) #Also change to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2309955"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reddit_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, I was right, but only slightly, w/ 2.4 mil after turning into sentences. Regardless, to continue exploring pre-processing steps I'll combine the two sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for line in reddit_sentences:\n",
    "    sentences.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4259979"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "but suggesting we raise the taxes on the wealthy will lead to more problems than it solves.\n",
      "borders said it is losing about $2 million a day at the stores it plans to close.\n",
      "the fact you can search for yours using fetchlands is really cool (and powerful).\n"
     ]
    }
   ],
   "source": [
    "#Lets see a few random samples\n",
    "print(sentences[50])\n",
    "print(sentences[50000])\n",
    "print(sentences[2500003])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Interesting this definetly seems like a good start for dealing with data from such different sources.\n",
    "\n",
    "The next thing I would like to try is a word lemmatizer... I have a feeling it's going to be a little too compuationally intense to run on such a large dataset, but if the benefit it provides is large enough it might be worth it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemma_test = []\n",
    "\n",
    "for sent in sentences:\n",
    "    words = nltk.word_tokenize(sent)\n",
    "\n",
    "    sentence = \"\"\n",
    "    \n",
    "    for word in words:\n",
    "        word = w_lem.lemmatize(word)\n",
    "        \n",
    "        sentence = sentence + \" \" + word\n",
    "        \n",
    "    lemma_test.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4259979"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lemma_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other option is to use stemming instead, so lets try that as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)  #Since well... english is the scope here, and lets ignore stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stem_test = []\n",
    "\n",
    "for sent in sentences:\n",
    "    words = nltk.word_tokenize(sent)\n",
    "\n",
    "    sentence = \"\"\n",
    "    \n",
    "    for word in words:\n",
    "        word = stemmer.stem(word)\n",
    "        \n",
    "        sentence = sentence + \" \" + word\n",
    "        \n",
    "    stem_test.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4259979"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stem_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this year 's `` what the bos make '' survey cover the period for fiscal year ending from march 31 to dec. 31 , 2008 .\n",
      " this year 's `` what the boss make '' survey cover the period for fiscal year end from march 31 to dec. 31 , 2008 .\n",
      " \n",
      " the rest of his family is spread out geographically -- his parent live in portland , along with one brother .\n",
      " the rest of his famili is spread out geograph -- his parent live in portland , along with one brother .\n",
      " \n",
      " in early november , filandrinos encouraged her to apply for another temporary position with bjc healthcare .\n",
      " in earli novemb , filandrino encourag her to appli for anoth temporari posit with bjc healthcar .\n"
     ]
    }
   ],
   "source": [
    "#As a sanity check lets compare a few line\n",
    "\n",
    "print(lemma_test[400])\n",
    "print(stem_test[400])\n",
    "print(\" \")\n",
    "print(lemma_test[405])\n",
    "print(stem_test[405])\n",
    "print(\" \")\n",
    "print(lemma_test[1400000])\n",
    "print(stem_test[1400000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results look pretty muchas expected, the two definitely produce different sentences, though the variance changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright now that I have 2 test sets pre-proccessed a little differently it is time to decide which one will work better. Rather then run the whole rest of the expiriment on both sets, and have to create two full sets, which would be too intensive/take too long, I am simply going to choose the option which gives me more results from a keyword search. I'll have to be careful here though to pre-processs the key words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-90dba517446e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk_w\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey_words_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Lemma Count for\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetLemmaCount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Stem Count for\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetStemCount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-90dba517446e>\u001b[0m in \u001b[0;36mgetLemmaCount\u001b[0;34m(key_words)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlemma_test\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_lem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcount_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m                 \u001b[0mcount_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#def getLemmaCount(key_words):\n",
    "    \n",
    "    count_list = []\n",
    "    \n",
    "    for line in lemma_test:\n",
    "        for key in key_words:\n",
    "            if ((w_lem.lemmatize(key) in line) and (line not in count_list)):\n",
    "                count_list.append(line)\n",
    "    \n",
    "    return len(count_list)\n",
    "\n",
    "def getStemCount(key_words):\n",
    "    \n",
    "    count_list = []\n",
    "    \n",
    "    for line in stem_test:\n",
    "        for key in key_words:\n",
    "            if ((stemmer.stem(key) in line) and (line not in count_list)):\n",
    "                count_list.append(line)\n",
    "    \n",
    "    return len(count_list)\n",
    "\n",
    "\n",
    "#Define some random key words\n",
    "key_words_list = [[\"monkey\",\"gorilla\",\"chimp\"],['sailor', 'seamen', 'seaman'], ['boat','vessel','ship']]\n",
    "\n",
    "\n",
    "#Don't actually run\n",
    "#for k_w in key_words_list:\n",
    "#    print(\"Lemma Count for\", k_w, \" : \", getLemmaCount(k_w))\n",
    "#    print(\"Stem Count for\", k_w, \" : \", getStemCount(k_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So before I actually run that chunk..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monkey\n",
      "monkey\n",
      "gorilla\n",
      "gorilla\n",
      "chimp\n",
      "chimp\n",
      "sailor\n",
      "sailor\n",
      "seaman\n",
      "seamen\n",
      "seaman\n",
      "seaman\n",
      "boat\n",
      "boat\n",
      "vessel\n",
      "vessel\n",
      "ship\n",
      "ship\n"
     ]
    }
   ],
   "source": [
    "key_words_list = [[\"monkey\",\"gorilla\",\"chimp\"],['sailor', 'seamen', 'seaman'], ['boat','vessel','ship']]\n",
    "for k_w in key_words_list:\n",
    "    for key in k_w:\n",
    "        print(w_lem.lemmatize(key))\n",
    "        print(stemmer.stem(key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah so... for most nouns or search query-esque words they will most likely be the same... so the verdict is using stemming, since it runs slightly faster for me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
